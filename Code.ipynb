{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akash1331/textSummarizer_major/blob/main/Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzvrXVUHusEZ"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "POS4rhrHusEc"
      },
      "outputs": [],
      "source": [
        "# from attention import AttentionLayer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "R9cT5MJzusEe"
      },
      "outputs": [],
      "source": [
        "# pip install ipykernal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "EwPklsu4usEe"
      },
      "outputs": [],
      "source": [
        "# !pip install numpy\n",
        "# !pip install pandas\n",
        "# !pip install tensorflow\n",
        "# !pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "vaQSHfyyusEf"
      },
      "outputs": [],
      "source": [
        "# !pip install bs4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "JEGv-ixAusEf"
      },
      "outputs": [],
      "source": [
        "# !pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.python.keras.layers import Layer\n",
        "from tensorflow.python.keras import backend as K\n",
        "\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "     \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n",
        "            if verbose:\n",
        "                print('wa.s>',W_a_dot_s.shape)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>',U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        def create_inital_state(inputs, hidden_size):\n",
        "            # We are not using initial states, but need to pass something to K.rnn funciton\n",
        "            fake_state = K.zeros_like(inputs)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "            fake_state = K.sum(fake_state, axis=[1, 2])  # <= (batch_size)\n",
        "            fake_state = K.expand_dims(fake_state)  # <= (batch_size, 1)\n",
        "            fake_state = K.tile(fake_state, [1, hidden_size])  # <= (batch_size, latent_dim\n",
        "            return fake_state\n",
        "\n",
        "        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n",
        "        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]\n"
      ],
      "metadata": {
        "id": "2hC4jUmYyrMc"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "xrIJNiDdusEg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import warnings\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqo416chusEg"
      },
      "source": [
        "# Data Read"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "2qaCEkN3usEg",
        "outputId": "128db35d-14df-4684-a181-591122c0a793"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: EOF inside string starting at row 156514",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-94-c746d595cea7>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# data=pd.read_csv(\"Dataset.csv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Reviews.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1776\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: EOF inside string starting at row 156514"
          ]
        }
      ],
      "source": [
        "# data=pd.read_csv(\"Dataset.csv\")\n",
        "data=pd.read_csv(\"Reviews.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvaNz08fusEg"
      },
      "outputs": [],
      "source": [
        "data.head(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mpdtRBIusEh"
      },
      "outputs": [],
      "source": [
        "data.drop_duplicates(subset=['Text'],inplace=True)\n",
        "#subset=['Text'] searches for duplicates only in the column with name Text(Last column)\n",
        "#inplace=true will cause all the rows which have same text value to be dropped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TF2IgLoMusEh"
      },
      "outputs": [],
      "source": [
        "data.dropna(axis=0,inplace=True)\n",
        "#this is the instruction to delete all rows with atleast one NaN values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd8tkjObusEh"
      },
      "source": [
        "# Data Info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAd39dy6usEh"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdVd6bsIusEh"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOdaBeBhusEi"
      },
      "outputs": [],
      "source": [
        "# To remove unnecessary symbols we will define a dictionary for expanding the contractions\n",
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "di8H8qyFusEi"
      },
      "outputs": [],
      "source": [
        "#Stop Words: A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that\n",
        "#a search engine has been programmed to ignore, both when indexing entries for searching and\n",
        "#when retrieving them as the result of a search query.\n",
        "#To check the list of stopwords we use the following instruction\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "945E_XO8usEi"
      },
      "outputs": [],
      "source": [
        "stop_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBvmFXC1usEi"
      },
      "source": [
        "Defining function for test cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "islEIKhIusEj"
      },
      "outputs": [],
      "source": [
        "def text_cleaner(text,num):\n",
        "    String1 = text.lower() #converting to lower case. After this the complete review will be in lower case\n",
        "    String1 = BeautifulSoup(String1, \"lxml\").text\n",
        "    #Beautiful Soup is a Python library for pulling data out of HTML and XML files. It is used for tasks like extracting the\n",
        "    #entire text from a page, extracting all URLs found in a page\n",
        "    #We create a BeautifulSoup object by passing two arguments:newString(raw HTML content) and lxml(HTML parser we want to use)\n",
        "    String1 = re.sub(r'\\([^)]*\\)', '', String1)\n",
        "    #The re.sub() function in the re module can be used to replace substrings.\n",
        "    #The syntax for re.sub() is re.sub(pattern,repl,string).\n",
        "    #That will replace the matches in string with repl.\n",
        "    String1 = re.sub('\"','', String1)\n",
        "    String1 = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in String1.split(\" \")])\n",
        "    #The join() method is a string method and returns a string in which the elements of sequence have been joined by str separator.\n",
        "    #Here we join with an empty string.\n",
        "    #The above instruction removes contraction from the string.\n",
        "    String1 = re.sub(r\"'s\\b\",\"\",String1)\n",
        "    String1 = re.sub(\"[^a-zA-Z]\", \" \", String1)\n",
        "    String1 = re.sub('[m]{2,}', 'mm', String1)\n",
        "    #removes the stopwords\n",
        "    #tokens will be a list\n",
        "    if(num==0):\n",
        "        #for text remove the stop_words\n",
        "        tokens = [w for w in String1.split() if not w in stop_words]\n",
        "    else:\n",
        "        #for summary stop words cannot be removed because the summary is already small. So just take all words in summary as tokens\n",
        "        tokens=String1.split()\n",
        "    long_words=[]\n",
        "    for i in tokens:\n",
        "        #for each token if length of the token is less than one then eliminate the token/word\n",
        "        if len(i)>1:\n",
        "            long_words.append(i)\n",
        "    #join will convert the list back to string and strip() will remove leading spaces if any.\n",
        "    return (\" \".join(long_words)).strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WI85BoFmusEj"
      },
      "source": [
        "Understanding the function text_cleaner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93PPPiArusEk"
      },
      "outputs": [],
      "source": [
        "#Sample string to understand the use of contraction mapping and join function\n",
        "string=\"ABC ain't def ain't\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28hpfaw2usEk"
      },
      "outputs": [],
      "source": [
        "#split the words of a sentence at the \" \" (string.split(\" \"))\n",
        "#check each word if it is the key of the dictionary contraction_mapping then replace the key by the value\n",
        "#if not then keep the word as it is\n",
        "#string will be list of resultant words\n",
        "string =[contraction_mapping[t] if t in contraction_mapping else t for t in string.split(\" \")]\n",
        "string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wr1Tc8x3usEl"
      },
      "outputs": [],
      "source": [
        "#to get the string back from list of words\n",
        "string=' '.join(string)\n",
        "string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z36pDBaXusEl"
      },
      "outputs": [],
      "source": [
        "#Join the list of words so obtanined with an empty string to convert back to string\n",
        "string = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in string.split(\" \")])\n",
        "string\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPzeLGVwusEl"
      },
      "outputs": [],
      "source": [
        "string\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCfBisvOusEl"
      },
      "outputs": [],
      "source": [
        "string.split() #string bydefault splits at spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0nJzoumusEl"
      },
      "outputs": [],
      "source": [
        "#This gives words which are not stopwords\n",
        "tokens = [w for w in string.split() if not w in stop_words]\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUlMnH8XusEm"
      },
      "outputs": [],
      "source": [
        "#redefining tokens to understand elimination of short words\n",
        "tokens=['I','am','a','girl']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkwxGdsCusEm"
      },
      "outputs": [],
      "source": [
        "long_words=[]\n",
        "for i in tokens :\n",
        "    #Initially long words is a empty list\n",
        "    #Examine each token and if its length is greater than 1 then include it in the long_word list.\n",
        "    #In this way all the short words with length 0 or 1 are removed\n",
        "    if len(i)>1:\n",
        "        long_words.append(i)\n",
        "long_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dQQq2gWusEm"
      },
      "outputs": [],
      "source": [
        "\" \".join(long_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iT-IzBcwusEm"
      },
      "outputs": [],
      "source": [
        "\" \".join(long_words).strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMGfmCekusEn"
      },
      "source": [
        "Calling the function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjUP4kUiusEn"
      },
      "outputs": [],
      "source": [
        "#call the function\n",
        "#cleaned_text is an empty string intially. For each entry i.e. row the text column value is\n",
        "#taken and cleaned by the function text_cleaner defined above. The cleaned text is added in the cleaned_text list.\n",
        "cleaned_text = []\n",
        "for t in data['Text']:\n",
        "    cleaned_text.append(text_cleaner(t,0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pPrbaSiusEn"
      },
      "outputs": [],
      "source": [
        "#The same function is called for the Summary column as well and in similar manner cleanned_summary list is generated.\n",
        "cleaned_summary = []\n",
        "for t in data['Summary']:\n",
        "    cleaned_summary.append(text_cleaner(t,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7e5fvArusEn"
      },
      "outputs": [],
      "source": [
        "cleaned_text[0:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZNzGHWCusEn"
      },
      "outputs": [],
      "source": [
        "cleaned_summary[0:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kviCeolNusEo"
      },
      "outputs": [],
      "source": [
        "#Adding two new columns namely cleaned_text and cleaned_summary in the data\n",
        "data['cleaned_text']=cleaned_text\n",
        "data['cleaned_summary']=cleaned_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrU46Q7cusEo"
      },
      "source": [
        "Drop ' '(Empty) rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Zr37JGRusEt"
      },
      "outputs": [],
      "source": [
        "#first replace blank spaces with NaN and then drop rows with NaN.\n",
        "#This can be called a trick to drop ' ' by using dropna\n",
        "data.replace('', np.nan, inplace=True)\n",
        "data.dropna(axis=0,inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBDITpR0usEt"
      },
      "source": [
        "\n",
        "# Understanding the distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsbCi_53usEu"
      },
      "outputs": [],
      "source": [
        "#For plotting graphs\n",
        "import matplotlib.pyplot as plt\n",
        "text_word_count = []\n",
        "summary_word_count = []\n",
        "for i in data['cleaned_text']:\n",
        "    #for each entry in cleaned_text the number of words are counted in the entry and the count is appended to text_word_count list\n",
        "    text_word_count.append(len(i.split()))\n",
        "for i in data['cleaned_summary']:\n",
        "    #for each entry in cleaned_summary the number of words are counted in the entry and the count is appended to summary_word_count list\n",
        "    summary_word_count.append(len(i.split()))\n",
        "#A dataframe with two columns is made. 1st column has entries of text_word_count and 2nd has entries of summart_word_count\n",
        "df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghIymwpmusEu"
      },
      "outputs": [],
      "source": [
        "df.head() #by default 5 entries are displayed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLfouNpVusEu"
      },
      "outputs": [],
      "source": [
        "#Plotting the histogram for the dataframe\n",
        "#Histogram plots the graph between values and frequencies\n",
        "df.hist(bins = 30)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXnhJqcxusEu"
      },
      "source": [
        "Lets find out the percentage of summaries below length=8,length=9 and length=10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcOWBR5PusEu"
      },
      "source": [
        "Find the appropriate max summary length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOT7Yc9SusEu"
      },
      "outputs": [],
      "source": [
        "#initialize count with value 0\n",
        "#for each entry in cleaned summary, split the cleaned summary at spaces to find number of words and if the no. of word\n",
        "#are less than or equal to 8 then increement count\n",
        "#In this way count will have the count of cleaned summaries with number of words less than or equal to 8.\n",
        "count=0\n",
        "for i in data['cleaned_summary']:\n",
        "    if(len(i.split())<=8):\n",
        "        count=count+1\n",
        "print(count/len(data['cleaned_summary']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zhag7LFeusEv"
      },
      "outputs": [],
      "source": [
        "#initialize count with value 0\n",
        "#for each entry in cleaned summary, split the cleaned summary at spaces to find number of words and if the no. of word\n",
        "#are less than or equal to 9 then increement count\n",
        "#In this way count will have the count of cleaned summaries with number of words less than or equal to 9.\n",
        "count=0\n",
        "for i in data['cleaned_summary']:\n",
        "    if(len(i.split())<=9):\n",
        "        count=count+1\n",
        "print(count/len(data['cleaned_summary']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ar3NBEc3usEv"
      },
      "outputs": [],
      "source": [
        "#initialize count with value 0\n",
        "#for each entry in cleaned summary, split the cleaned summary at spaces to find number of words and if the no. of word\n",
        "#are less than or equal to 10 then increement count\n",
        "#In this way count will have the count of cleaned summaries with number of words less than or equal to 10.\n",
        "count=0\n",
        "for i in data['cleaned_summary']:\n",
        "    if(len(i.split())<=10):\n",
        "        count=count+1\n",
        "print(count/len(data['cleaned_summary']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJafJG1pusEv"
      },
      "source": [
        "Lets fix the max cleaned summary length to 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_SHdIZausEv"
      },
      "source": [
        "Find the appropriate max text length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5bmr4_JusEv"
      },
      "outputs": [],
      "source": [
        "#initialize count with value 0\n",
        "#foe each entry in cleaned text, split the cleaned text at spaces to find number of words and if the no. of word\n",
        "#are less than or equal to 20 then increement count\n",
        "#In this way count will have the count of cleaned texts with number of words less than or equal to 20.\n",
        "count=0\n",
        "for i in data['cleaned_text']:\n",
        "    if(len(i.split())<=20):\n",
        "        count=count+1\n",
        "print(count/len(data['cleaned_text']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoN0AF1MusEv"
      },
      "outputs": [],
      "source": [
        "#initialize count with value 0\n",
        "#foe each entry in cleaned text, split the cleaned text at spaces to find number of words and if the no. of word\n",
        "#are less than or equal to 25 then increement count\n",
        "#In this way count will have the count of cleaned texts with number of words less than or equal to 25.\n",
        "count=0\n",
        "for i in data['cleaned_text']:\n",
        "    if(len(i.split())<=25):\n",
        "        count=count+1\n",
        "print(count/len(data['cleaned_text']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVX_rbY7usEw"
      },
      "outputs": [],
      "source": [
        "#initialize count with value 0\n",
        "#foe each entry in cleaned text, split the cleaned text at spaces to find number of words and if the no. of word\n",
        "#are less than or equal to 35 then increement count\n",
        "#In this way count will have the count of cleaned texts with number of words less than or equal to 35.\n",
        "count=0\n",
        "for i in data['cleaned_text']:\n",
        "    if(len(i.split())<=35):\n",
        "        count=count+1\n",
        "print(count/len(data['cleaned_text']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlR6r2JMusEw"
      },
      "outputs": [],
      "source": [
        "#initialize count with value 0\n",
        "#foe each entry in cleaned text, split the cleaned text at spaces to find number of words and if the no. of word\n",
        "#are less than or equal to 45 then increement count\n",
        "#In this way count will have the count of cleaned texts with number of words less than or equal to 45.\n",
        "count=0\n",
        "for i in data['cleaned_text']:\n",
        "    if(len(i.split())<=45):\n",
        "        count=count+1\n",
        "print(count/len(data['cleaned_text']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7gp1ulDusEw"
      },
      "source": [
        "Lets fix the max text length as 45"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-O8NncyRusEw"
      },
      "source": [
        "Now selecting those entries in which cleaned text length is less than equal to 45 and cleaned summary length is less than equal to 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYsCpnp0usEw"
      },
      "outputs": [],
      "source": [
        "max_summary_len=10\n",
        "max_text_len=45"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7t2AKWmHusEx"
      },
      "outputs": [],
      "source": [
        "#making array of cleaned text entries and cleaned summary entries\n",
        "cleaned_text =np.array(data['cleaned_text'])\n",
        "cleaned_summary=np.array(data['cleaned_summary'])\n",
        "\n",
        "#short_text and short_summary are initially empty but will contain all the text and summary entries which fall in the desired range\n",
        "short_text=[]\n",
        "short_summary=[]\n",
        "\n",
        "for i in range(len(cleaned_text)):\n",
        "    #For all entries if the cleaned_summary has no. of words <=max summary length which is equal to 10\n",
        "    #and cleaned_text has no. of words <=max text length which is equal to 45 add such entries to the lists short_text and short_summary\n",
        "    if(len(cleaned_summary[i].split())<=max_summary_len and len(cleaned_text[i].split())<=max_text_len):\n",
        "        short_text.append(cleaned_text[i])\n",
        "        short_summary.append(cleaned_summary[i])\n",
        "#create a dataframe to store the results of short_text and short_summary\n",
        "df1=pd.DataFrame({'text':short_text,'summary':short_summary})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dLgBfb-usEx"
      },
      "outputs": [],
      "source": [
        "df1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUqcxc3BusEx"
      },
      "source": [
        "Now add the start and end token to each summary. This can be done using lambda function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNs0b65yusEx"
      },
      "outputs": [],
      "source": [
        "#This will replace each summary with 'starttoken' as start token concatenated with summary concatenated with 'endtoken' as end token\n",
        "#Be sure that the chosen special tokens never appear in the summary\n",
        "df1['summary'] = df1['summary'].apply(lambda x : 'starttoken '+ x + ' endtoken')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oIDoyjZusEx"
      },
      "source": [
        "Now splitting data into training and testing sets. Take 90% of the dataset as the training data and evaluate the performance on the remaining 10%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvCNLmUuusEy"
      },
      "outputs": [],
      "source": [
        "#Sklearn is used to perform the split. This is standard technique to split the dataset.Test size is set to 0.1 i.e. 10%.\n",
        "#x variable is text\n",
        "#y variable is summary\n",
        "#df['text'] and df['summary'] contain respective reviews and summaries in form of array\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test=train_test_split(np.array(df1['text']),np.array(df1['summary']),test_size=0.1,random_state=0,shuffle=True)\n",
        "#xtrain,x_test,y_train,y_test all are numpy arrays containing reviews and summaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ5B9EEBusEy"
      },
      "source": [
        "# Preparing the Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88ROMtTZusEy"
      },
      "outputs": [],
      "source": [
        "check=np.array([\"This is a great product for money. I will recommend you to buy this\"])\n",
        "check[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcnQ1_tQusEy"
      },
      "source": [
        "# Text Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtSQIaGDusEy"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwOumFnIusEy"
      },
      "outputs": [],
      "source": [
        "#prepare a tokenizer for reviews on training data\n",
        "t = Tokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKVA_YNVusEz"
      },
      "outputs": [],
      "source": [
        "t\n",
        "#Output would be  <keras_preprocessing.text.Tokenizer at 0x217e980deb8>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcXaEqi3usEz"
      },
      "outputs": [],
      "source": [
        "#tokenizing x_train. First x_train which is a numpy array is converted to list\n",
        "t.fit_on_texts(list(x_train))\n",
        "#fit_on_texts Updates internal vocabulary based on a list of texts.\n",
        "#This method creates the vocabulary index based on word frequency.\n",
        "#So if you give it something like, \"The cat sat on the mat.\"\n",
        "#It will create a dictionary s.t. word_index[\"the\"] = 1;\n",
        "#word_index[\"cat\"] = 2 it is word -> index dictionary so every word gets a unique integer value.\n",
        "#0 is reserved for padding.\n",
        "#So lower integer means more frequent word (often the first few are stop words because they appear a lot)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "555B_vz0usEz"
      },
      "source": [
        "Rarewords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "risM6XjpusEz"
      },
      "outputs": [],
      "source": [
        "#Rare words are those words which do not appear too often\n",
        "#Defining the threshold as 3. If the words apear less than thrice then the word are rare words.\n",
        "threshold=3\n",
        "#count has count of rare words\n",
        "count=0\n",
        "#totalcount has the count of total number of words i.e. size of vocabulary\n",
        "totalcount=0\n",
        "#frequency has total frequency of all the rare words\n",
        "frequency=0\n",
        "#totalfrequency has the sum of all frequencies of all words\n",
        "totalfrequency=0\n",
        "#t.word_counts.items() will give items of ordered dictionary i.e.  'key' and 'value' pair. key being the word and value being the number of times it ocuured.\n",
        "#odict_items([('love', 45148), ('raspberry', 1096), ('shortbread', 276), ('cookies', 6596), ('easy', 10588), ('find', 21556), ....\n",
        "for key,value in t.word_counts.items():\n",
        "    #accessing each key value pair\n",
        "    #totalcount is increemented by 1 as the word encountered is a new word add totalcount by 1\n",
        "    totalcount=totalcount+1\n",
        "    #totalfrequency is increemneted by value\n",
        "    totalfrequency=totalfrequency+value\n",
        "    #if value is less than threshold than it is rare word and count is incremented by 1 and frequency by value.\n",
        "    if(value<threshold):\n",
        "        count=count+1\n",
        "        frequency=frequency+value\n",
        "print(count)\n",
        "print(totalcount)\n",
        "# %of rare words is (number of rare words divided by total number of words) multiplied by 100 i.e. (count divided by totalcount) multiplied by 100\n",
        "print(\"% of rare words in vocabulary:\",(count/totalcount)*100)\n",
        "# coverage of rare words is (frequency divided by total frequency) multiplied by 100\n",
        "print(\"Total Coverage of rare words:\",(frequency/totalfrequency)*100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRfD7hgyusEz"
      },
      "outputs": [],
      "source": [
        "#prepare a tokenizer for reviews on training data\n",
        "#totalcount-count is number of common words\n",
        "#Only common words will be remembered\n",
        "x_tokenizer = Tokenizer(num_words=totalcount-count)\n",
        "x_tokenizer.fit_on_texts(list(x_train))\n",
        "#x_tokenizer.word_index will give\n",
        "#{'like': 1,'good': 2, 'great': 3,'taste': 4, 'product': 5,'love': 6,'one': 7,....\n",
        "#This means like appers is the most common word followed by good then great and so on\n",
        "\n",
        "#convert text sequences into integer sequences\n",
        "x_train_sequence    =   x_tokenizer.texts_to_sequences(x_train)\n",
        "#only common words will be remembered\n",
        "x_test_sequence   =   x_tokenizer.texts_to_sequences(x_test)\n",
        "\n",
        "#pad_sequences is used to ensure that all sequences in a list have the same length.\n",
        "#By default this is done by padding 0 in the beginning of each sequence until each sequence has the same length as the longest sequence.\n",
        "#Here zeros are padded at the end\n",
        "x_train   =   pad_sequences(x_train_sequence,  maxlen=max_text_len,padding='post')\n",
        "x_test   =   pad_sequences(x_test_sequence, maxlen=max_text_len, padding='post')\n",
        "\n",
        "#size of vocabulary ( +1 for padding token)\n",
        "x_voc   =  x_tokenizer.num_words + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YyEXZuJusE0"
      },
      "outputs": [],
      "source": [
        "x_voc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FjfY-3lusE0"
      },
      "source": [
        "For understanding Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uo0jahxtusE0"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "texts = ['a a a', 'b b b b b', 'c c c c c c c','ddd','aa a','aa aa']\n",
        "#'a a a' is a string with 3 words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wV4sGyousE0"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(num_words=4)\n",
        "#num_words: the maximum number of words to keep, based on word frequency.\n",
        "#Only the most common num_words-1 words will be kept.\n",
        "#Tokenizer will use only three most common words and at the same time, it will keep the counter of all words - even when it's obvious that it will not use it later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35vyb2byusE0"
      },
      "outputs": [],
      "source": [
        "tokenizer.fit_on_texts(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0q2SAndusE1"
      },
      "outputs": [],
      "source": [
        "tokenizer.word_index\n",
        "#c is the most common word so will get the value 1.\n",
        "#b will get value 2\n",
        "#a will get 3\n",
        "#aa will get 4\n",
        "#ddd will get 5\n",
        "#More times a number appears lesser will be its key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rA_S0GZmusE1"
      },
      "outputs": [],
      "source": [
        "tokenizer.texts_to_sequences(texts)\n",
        "#only c,b,and a will be remembered\n",
        "#See \"aa a\" i.e. 4th index only a is remembered and aa is not so only 3 is the answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spOIgo9zusE1"
      },
      "source": [
        "# Summary Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p20LRwznusE1"
      },
      "outputs": [],
      "source": [
        "#prepare a tokenizer for reviews on training data\n",
        "t1= Tokenizer()\n",
        "t1.fit_on_texts(list(y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJCFfK-NusE1"
      },
      "outputs": [],
      "source": [
        "#Doing the similar thing with summary\n",
        "#Threshold is set to 5\n",
        "threshold=5\n",
        "count=0\n",
        "totalcount=0\n",
        "frequency=0\n",
        "totalfrequency=0\n",
        "\n",
        "for key,value in t.word_counts.items():\n",
        "    totalcount=totalcount+1\n",
        "    totalfrequency=totalfrequency+value\n",
        "    if(value<threshold):\n",
        "        count=count+1\n",
        "        frequency=frequency+value\n",
        "print(count)\n",
        "print(totalcount)\n",
        "print(\"% of rare words in vocabulary:\",(count/totalcount)*100)\n",
        "print(\"Total Coverage of rare words:\",(frequency/totalfrequency)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbqyrsZmusE2"
      },
      "outputs": [],
      "source": [
        "#prepare a tokenizer for reviews on training data\n",
        "y_tokenizer = Tokenizer(num_words=totalcount-count)\n",
        "y_tokenizer.fit_on_texts(list(y_train))\n",
        "\n",
        "#convert text sequences into integer sequences\n",
        "y_train_sequence=y_tokenizer.texts_to_sequences(y_train)\n",
        "y_test_sequence=y_tokenizer.texts_to_sequences(y_test)\n",
        "\n",
        "#padding zero upto maximum length\n",
        "y_train=pad_sequences(y_train_sequence, maxlen=max_summary_len, padding='post')\n",
        "y_test=pad_sequences(y_test_sequence, maxlen=max_summary_len, padding='post')\n",
        "\n",
        "#size of vocabulary\n",
        "y_voc=y_tokenizer.num_words +1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I84lqSNHusE2"
      },
      "outputs": [],
      "source": [
        "x_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aoie0xYusE2"
      },
      "outputs": [],
      "source": [
        "y_voc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMrFKTJmusE3"
      },
      "outputs": [],
      "source": [
        "#The number of times startoken appears should be equal to length of training data\n",
        "y_tokenizer.word_counts['starttoken'],len(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NYHk52musE3"
      },
      "outputs": [],
      "source": [
        "#Deleting those rows which only contain start and end token\n",
        "empty=[]\n",
        "#Checking each element of y train. Each element of y train is a list in itself.\n",
        "for i in range(len(y_train)):\n",
        "    count=0\n",
        "    for j in y_train[i]:\n",
        "        #checking each element in one element of y_train\n",
        "            count=count+1\n",
        "    if(count==2):\n",
        "        #if there are only 2 non zero elements that is start and end token then the list is actualy empty and append that index\n",
        "        #in empty list so that we can delete those rows\n",
        "        empty.append(i)\n",
        "\n",
        "#Deleting x and y  for those indices present in empty list that is those rows which only have start and end token\n",
        "y_train=np.delete(y_train,empty, axis=0)\n",
        "x_train=np.delete(x_train,empty, axis=0)\n",
        "#Axis is 0 because rows have to be deleted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqtGq8PXusE3"
      },
      "outputs": [],
      "source": [
        "#Deleting those rows which only contain start and end token\n",
        "empty=[]\n",
        "for i in range(len(y_test)):\n",
        "    count=0\n",
        "    for j in y_test[i]:\n",
        "        if j!=0:\n",
        "            count=count+1\n",
        "    if(count==2):\n",
        "        empty.append(i)\n",
        "\n",
        "y_test=np.delete(y_test,empty, axis=0)\n",
        "x_test=np.delete(x_test,empty, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBAsQR81usE4"
      },
      "source": [
        "# Model Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9S925yOSusE4"
      },
      "outputs": [],
      "source": [
        "#Return Sequences = True: When the return sequences parameter is set to True, LSTM produces the hidden state and cell state for every timestep\n",
        "\n",
        "#Return State = True: When return state = True, LSTM produces the hidden state and cell state of the last timestep only\n",
        "\n",
        "#Initial State: This is used to initialize the internal states of the LSTM for the first timestep\n",
        "\n",
        "#Stacked LSTM: Stacked LSTM has multiple layers of LSTM stacked on top of each other. This leads to a better representation of the sequence. I encourage you to experiment with the multiple layers of the LSTM stacked on top of each other (it’s a great way to learn this)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deVkcbhpusE8"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "K.clear_session()\n",
        "\n",
        "latent_dim = 300\n",
        "embedding_dim=100\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_text_len,))\n",
        "\n",
        "#embedding layer\n",
        "enc_emb =  Embedding(x_voc, embedding_dim,trainable=True)(encoder_inputs)\n",
        "\n",
        "#encoder lstm 1\n",
        "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
        "\n",
        "#encoder lstm 2\n",
        "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
        "\n",
        "#encoder lstm 3\n",
        "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "#embedding layer\n",
        "dec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
        "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
        "\n",
        "# Attention layer\n",
        "import tensorflow as tf\n",
        "# attn_layer = AttentionLayer(name='attention_layer')\n",
        "attn_out = tf.keras.layers.Attention()([encoder_outputs, decoder_outputs])\n",
        "# attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
        "\n",
        "# Concat attention input and decoder LSTM output\n",
        "# decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "decoder_concat_input = Concatenate(axis=1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "#dense layer\n",
        "decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\n",
        "decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kO0bpwSwusE9"
      },
      "outputs": [],
      "source": [
        "# model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAsHHvW1usE9"
      },
      "outputs": [],
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)\n",
        "#If the validation loss increases then stop the model early"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n"
      ],
      "metadata": {
        "id": "sHRjOBQKw_y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3sVbLX2usE9"
      },
      "outputs": [],
      "source": [
        "history=model.fit([x_train,y_train[:,:-1]], y_train.reshape(y_train.shape[0],y_train.shape[1], 1)[:,1:] ,epochs=50,callbacks=[es],batch_size=128, validation_data=([x_test,y_test[:,:-1]], y_test.reshape(y_test.shape[0],y_test.shape[1], 1)[:,1:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCAy4zWEusE-"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.legend()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxMX_KYQusE-"
      },
      "outputs": [],
      "source": [
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "#As it is visible that validation loss decresed till 25 epochs and then it remained constant in 26th epoch so"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VnYEFeTusE-"
      },
      "outputs": [],
      "source": [
        "y_tokenizer.index_word\n",
        "#starttoken and endtoken have the most frequency because they appear in each summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nx2UvOXausE_"
      },
      "outputs": [],
      "source": [
        "x_tokenizer.index_word\n",
        "#'like' is most frequent in review followed by 'good' and so on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kA7zccn3usE_"
      },
      "outputs": [],
      "source": [
        "y_tokenizer.word_index\n",
        "#difference between word_index and index_word is that in word_index word comes first followed by index whereas in index_word it is the opposite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40vxEf_pusE_"
      },
      "outputs": [],
      "source": [
        "y_index_word=y_tokenizer.index_word\n",
        "x_index_word=x_tokenizer.index_word\n",
        "y_word_index=y_tokenizer.word_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKa9wpiTusE_"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHIogyiHusFA"
      },
      "outputs": [],
      "source": [
        "# For building the model 3 LSTM encoder layers were used along with decoder\n",
        "#Now model is decoded\n",
        "# Encode the input sequence to get the feature vector\n",
        "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
        "\n",
        "# Decoder setup\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n",
        "\n",
        "# Get the embeddings of the decoder sequence\n",
        "dec_emb2= dec_emb_layer(decoder_inputs)\n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "#attention inference\n",
        "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
        "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
        "\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
        "    [decoder_outputs2] + [state_h2, state_c2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIf9988nusFA"
      },
      "outputs": [],
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    #It is an array of one row and one column\n",
        "    target_seq = np.zeros((1,1))\n",
        "\n",
        "    # Populate the first word of target sequence with the start word.\n",
        "    #target_seq will be array of one row and one column with value as 1\n",
        "    target_seq[0, 0] = y_word_index['starttoken']\n",
        "    #initializing stop condition to be false\n",
        "    stop_condition = False\n",
        "    #decode sentence initially is empty\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = y_index_word[sampled_token_index]\n",
        "\n",
        "        if(sampled_token!='endtoken'):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "        # Exit condition: either hit max length or find stop word.\n",
        "        if (sampled_token == 'endtoken'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update internal states\n",
        "        e_h, e_c = h, c\n",
        "\n",
        "    return decoded_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7k1-IlvDusFA"
      },
      "source": [
        "Understanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UmBL4f4usFA"
      },
      "outputs": [],
      "source": [
        "target_seq = np.zeros((1,1))\n",
        "target_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLSCb9L4usFB"
      },
      "outputs": [],
      "source": [
        "target_seq[0, 0] = y_word_index['starttoken']\n",
        "target_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgFLx9aOusFB"
      },
      "outputs": [],
      "source": [
        "x=np.arange(8)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mx5x18VFusFB"
      },
      "outputs": [],
      "source": [
        "x.reshape(1,8)\n",
        "# 1 D array converted to 2D array with one row and 8 columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTdoQDaYusFB"
      },
      "outputs": [],
      "source": [
        "def seq2summary(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if((i!=0 and i!=y_word_index['starttoken']) and i!=y_word_index['endtoken']):\n",
        "            newString=newString+y_index_word[i]+' '\n",
        "    return newString\n",
        "\n",
        "def seq2text(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if(i!=0):\n",
        "            newString=newString+x_index_word[i]+' '\n",
        "    return newString"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1DVAA5GusFB"
      },
      "outputs": [],
      "source": [
        "for i in range(0,100):\n",
        "    print(\"Review:\",seq2text(x_train[i]))\n",
        "    print(\"Original summary:\",seq2summary(y_train[i]))\n",
        "    print(\"Predicted summary:\",decode_sequence(x_train[i].reshape(1,max_text_len)))\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhMy4kMhusFC"
      },
      "outputs": [],
      "source": [
        "for i in range(0,100):\n",
        "    print(\"Review:\",seq2text(x_test[i]))\n",
        "    print(\"Original summary:\",seq2summary(y_test[i]))\n",
        "    print(\"Predicted summary:\",decode_sequence(x_test[i].reshape(1,max_text_len)))\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eD7sKhXnusFC"
      },
      "outputs": [],
      "source": [
        "check=np.array([\"This is a great product for money. I ordered it for my sister and she loved the product. She is very happy and so am I\"])\n",
        "check_seq=x_tokenizer.texts_to_sequences(check)\n",
        "check_test=pad_sequences(check_seq, maxlen=max_text_len, padding='post')\n",
        "print(\"Predicted summary:\",decode_sequence(check_test[0].reshape(1,max_text_len)))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}